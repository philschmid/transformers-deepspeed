{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate GPT inference with DeepSpeed-Inference on GPUs\n",
    "\n",
    "In this session, you will learn how to optimize GPT-2/GPT-J for Inerence using [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) and [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/). The session will show you how to apply state-of-the-art optimization techniques using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/). \n",
    "This session will focus on single GPU inference for GPT-2, GPT-NEO and GPT-J like models\n",
    "By the end of this session, you will know how to optimize your Hugging Face Transformers models (GPT-2, GPT-J) using DeepSpeed-Inference. We are going to optimize GPT-j 6B for text-generation.\n",
    "\n",
    "You will learn how to:\n",
    "1. [Setup Development Environment](#1-Setup-Development-Environment)\n",
    "2. [Load vanilla GPT-J model and set baseline](#2-Load-vanilla-GPT-J-model-and-set-baseline)\n",
    "3. [Optimize GPT-J for GPU using DeepSpeeds `InferenceEngine`](#3-Optimize-GPT-J-for-GPU-using-DeepSpeeds-InferenceEngine)\n",
    "4. [Evaluate the performance and speed](#4-Evaluate-the-performance-and-speed)\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "_This tutorial was created and run on a g4dn.2xlarge AWS EC2 Instance including an NVIDIA T4._\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Intro: What is DeepSpeed-Inference\n",
    "\n",
    "[DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/) is an extension of the [DeepSpeed](https://www.deepspeed.ai/) framework focused on inference workloads.  [DeepSpeed Inference](https://www.deepspeed.ai/#deepspeed-inference) combines model parallelism technology such as tensor, pipeline-parallelism, with custom optimized cuda kernels.\n",
    "DeepSpeed provides a seamless inference mode for compatible transformer based models trained using DeepSpeed, Megatron, and HuggingFace. For a list of compatible models please see [here](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/replace_policy.py).\n",
    "As mentioned DeepSpeed-Inference integrates model-parallelism techniques allowing you to run multi-GPU inference for LLM, like [BLOOM](https://huggingface.co/bigscience/bloom) with 176 billion parameters.\n",
    "If you want to learn more about DeepSpeed inference: \n",
    "* [Paper: DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale](https://arxiv.org/pdf/2207.00032.pdf)\n",
    "* [Blog: Accelerating large-scale model inference and training via system optimizations and compression](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Deepspeed, along with PyTorch, Transfromers and some other libraries. Running the following cell will install all the required packages.\n",
    "\n",
    "_Note: You need a machine with a GPU and a compatible CUDA installed. You can check this by running `nvidia-smi` in your terminal. If your setup is correct, you should get statistics about your GPU._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.11.0 torchvision==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113 --upgrade -q \n",
    "# !pip install deepspeed==0.7.2 --upgrade -q \n",
    "\n",
    "!pip install transformers[sentencepiece]==4.21.2 accelerate --upgrade -q \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/microsoft/DeepSpeed.git@ds-inference/support-large-token-length\n",
      "  Cloning https://github.com/microsoft/DeepSpeed.git (to revision ds-inference/support-large-token-length) to /tmp/pip-req-build-cquulhsj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/DeepSpeed.git /tmp/pip-req-build-cquulhsj\n",
      "  Running command git checkout -b ds-inference/support-large-token-length --track origin/ds-inference/support-large-token-length\n",
      "  Switched to a new branch 'ds-inference/support-large-token-length'\n",
      "  Branch 'ds-inference/support-large-token-length' set up to track remote branch 'ds-inference/support-large-token-length' from 'origin'.\n",
      "  Resolved https://github.com/microsoft/DeepSpeed.git to commit 6d7c133d9d89f91f11cf5ebe3e35f2c919c008be\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hjson in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (3.1.0)\n",
      "Requirement already satisfied: ninja in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (1.10.2.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (1.22.4)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (5.9.1)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (8.0.0)\n",
      "Requirement already satisfied: pydantic in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (1.9.2)\n",
      "Requirement already satisfied: torch in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (1.11.0+cu113)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from deepspeed==0.7.3+6d7c133) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from packaging->deepspeed==0.7.3+6d7c133) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda/envs/dev/lib/python3.9/site-packages (from pydantic->deepspeed==0.7.3+6d7c133) (4.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.7.3+6d7c133-py3-none-any.whl size=738006 sha256=5df94c916fec3b1116bff83d3a4a99df33ba75e2cfa91edd93d9086ddf508086\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ik9vksw5/wheels/28/9e/de/8f6e92edccac775a100cf9b4266c787ec33f7ac332e1fd9cc9\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: deepspeed\n",
      "  Attempting uninstall: deepspeed\n",
      "    Found existing installation: deepspeed 0.7.2\n",
      "    Uninstalling deepspeed-0.7.2:\n",
      "      Successfully uninstalled deepspeed-0.7.2\n",
      "Successfully installed deepspeed-0.7.3+6d7c133\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/microsoft/DeepSpeed.git@ds-inference/support-large-token-length --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start. Let's make sure all packages are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch \n",
    "\n",
    "# check deepspeed installation\n",
    "report = !python3 -m deepspeed.env_report\n",
    "r = re.compile('.*ninja.*OKAY.*')\n",
    "assert any(r.match(line) for line in report) == True, \"DeepSpeed Inference not correct installed\"\n",
    "\n",
    "# check cuda and torch version\n",
    "torch_version, cuda_version = torch.__version__.split(\"+\")\n",
    "torch_version = \".\".join(torch_version.split(\".\")[:2])\n",
    "cuda_version = f\"{cuda_version[2:4]}.{cuda_version[4:]}\"\n",
    "r = re.compile(f'.*torch.*{torch_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Torch version\"\n",
    "r = re.compile(f'.*cuda.*{cuda_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Cuda version\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load vanilla GPT-J model and set baseline\n",
    "\n",
    "After we set up our environment, we create a baseline for our model. We use the [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B), a GPT-J 6B was trained on the [Pile](https://pile.eleuther.ai/), a large-scale curated dataset created by [EleutherAI](https://www.eleuther.ai/). This model was trained for 402 billion tokens over 383,500 steps on TPU v3-256 pod. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token correctly.\n",
    "\n",
    "To create our baseline, we load the model with `transformers` and run inference. \n",
    "\n",
    "_Note: We created a [separate repository](https://huggingface.co/philschmid/gpt-j-6B-fp16-sharded) containing sharded `fp16` weights to make it easier to load the models on smaller CPUs by using the `device_map` feature to automatically place sharded checkpoints on GPU. Learn more [here](https://huggingface.co/docs/accelerate/main/en/big_modeling#accelerate.cpu_offload)_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded on device cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Model Repository on huggingface.co\n",
    "model_id = \"philschmid/gpt-j-6B-fp16-sharded\"\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# we use device_map auto to automatically place all shards on the GPU to save CPU memory\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "print(f\"model is loaded on device {model.device.type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input payload: \n",
      " \n",
      "Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\n",
      "prediction: \n",
      " \n",
      "  doesn't look like it's coming at all.\n",
      "\n",
      "Best regards and have a nice weekend\n",
      "\n",
      "But if my client isn't here to sign the contract and confirm the appointment, then I must have some sort of notification from the client that they are coming, which he has not provided.\n",
      "\n",
      "If you do contact the client, then you would need to arrange to meet with them for some sort of a face to face meeting with a contract (to sign and confirm) or get confirmation of the appointment and the payment via email so that you have all the essential documentation and evidence in place before your meeting.\n",
      "\n",
      "It would be\n"
     ]
    }
   ],
   "source": [
    "payload = \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(payload,return_tensors=\"pt\").input_ids.to(model.device)\n",
    "print(f\"input payload: \\n \\n{payload}\")\n",
    "logits = model.generate(input_ids, do_sample=True, num_beams=1, min_length=128, max_new_tokens=128)\n",
    "\n",
    "print(f\"prediction: \\n \\n {tokenizer.decode(logits[0].tolist()[len(input_ids[0]):])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"My name is Philipp and I am the owner and founder of RumpusandRamblings.com and the RumpusandRamblings.net Blog. I will be handling the daily affairs of the site while the founder, Andrew, works on building the world's tiniest violin.\\n\\nA Quickie with an Ex: The Last Five Years\\n\\nThe RumpusandRamblings.com is a blog written by me, with contributions from Andrew, and sometimes\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model\n",
    "example = \"My name is Philipp and I\"\n",
    "input_ids = tokenizer(example,return_tensors=\"pt\").input_ids.to(model.device)\n",
    "logits = model.generate(input_ids, do_sample=True, max_length=100)\n",
    "tokenizer.decode(logits[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a latency baseline we use the `measure_latency` function, which implements a simple python loop to run inference and calculate the avg, mean & p95 latency for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "import transformers\n",
    "# hide generation warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def measure_latency(model, tokenizer, payload, generation_args={},device=model.device):\n",
    "    input_ids = tokenizer(payload, return_tensors=\"pt\").input_ids.to(device)\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(2):\n",
    "        _ =  model.generate(input_ids, **generation_args)\n",
    "    # Timed run\n",
    "    for _ in range(10):\n",
    "        start_time = perf_counter()\n",
    "        _ = model.generate(input_ids, **generation_args)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use greedy search as decoding strategy and will generate 128 new tokens with 128 tokens as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 128\n",
      "Vanilla model: P95 latency (ms) - 8931.68983990006; Average latency (ms) - 8923.24 +\\- 6.33;\n"
     ]
    }
   ],
   "source": [
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"*2\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# generation arguments\n",
    "generation_args = dict(\n",
    "  do_sample=False,\n",
    "  num_beams=1,\n",
    "  min_length=128,\n",
    "  max_new_tokens=128\n",
    ")\n",
    "vanilla_results = measure_latency(model,tokenizer,payload,generation_args)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model achieves latency of `8.9s` for `128` tokens or `69ms/token`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Optimize GPT-J for GPU using DeepSpeeds `InferenceEngine`\n",
    "\n",
    "The next and most important step is to optimize our model for GPU inference. This will be done using the DeepSpeed `InferenceEngine`. The `InferenceEngine` is initialized using the `init_inference` method. The `init_inference` method expects as parameters atleast:\n",
    "\n",
    "* `model`: The model to optimize.\n",
    "* `mp_size`: The number of GPUs to use.\n",
    "* `dtype`: The data type to use.\n",
    "* `replace_with_kernel_inject`: Whether inject custom kernels.\n",
    "\n",
    "You can find more information about the `init_inference` method in the [DeepSpeed documentation](https://deepspeed.readthedocs.io/en/latest/inference-init.html) or [thier inference blog](https://www.deepspeed.ai/tutorials/inference-tutorial/).\n",
    "\n",
    "_Note: You might need to restart your kernel if you are running into a CUDA OOM error._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-01 15:52:45,271] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.7.3+6d7c133, git-hash=6d7c133, git-branch=ds-inference/support-large-token-length\n",
      "[2022-09-01 15:52:45,272] [INFO] [logging.py:68:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/ubuntu/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py39_cu113/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 1.179579734802246 seconds\n",
      "[2022-09-01 15:52:47,886] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 16384, 'heads': 16, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 64, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': False, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False}\n",
      "model is loaded on device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import deepspeed\n",
    "\n",
    "# Model Repository on huggingface.co\n",
    "model_id = \"philschmid/gpt-j-6B-fp16-sharded\"\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "\n",
    "# init deepspeed inference engine\n",
    "ds_model = deepspeed.init_inference(\n",
    "    model=model,      # Transformers models\n",
    "    mp_size=1,        # Number of GPU\n",
    "    dtype=torch.float16, # dtype of the weights (fp16)\n",
    "    replace_method=\"auto\", # Lets DS autmatically identify the layer to replace\n",
    "    replace_with_kernel_inject=True, # replace the model with the kernel injector\n",
    ")\n",
    "print(f\"model is loaded on device {ds_model.module.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect our model graph to see that the vanilla `GPTJLayer` has been replaced with an `HFGPTJLayer`, which includes the `DeepSpeedTransformerInference` module, a custom `nn.Module` that is optimized for inference by the DeepSpeed Team.\n",
    "\n",
    "```python\n",
    "InferenceEngine(\n",
    "  (module): GPTJForCausalLM(\n",
    "    (transformer): GPTJModel(\n",
    "      (wte): Embedding(50400, 4096)\n",
    "      (drop): Dropout(p=0.0, inplace=False)\n",
    "      (h): ModuleList(\n",
    "        (0): DeepSpeedTransformerInference(\n",
    "          (attention): DeepSpeedSelfAttention()\n",
    "          (mlp): DeepSpeedMLP()\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed.ops.transformer.inference import DeepSpeedTransformerInference\n",
    "\n",
    "assert isinstance(ds_model.module.transformer.h[0], DeepSpeedTransformerInference) == True, \"Model not sucessfully initalized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My name is Philipp and I use my blog to talk about a lot of things that happen in the news or that make my life sadder. If you want to know more about me and my blog read this.\\n\\nPosts navigation\\n\\nIâ€™m a fan of good quality and well edited videos as much as Iâ€™m a fan of long and detailed blog posts. So Iâ€™m really happy that the web is full of a variety of quality content today, even if'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model\n",
    "example = \"My name is Philipp and I\"\n",
    "input_ids = tokenizer(example,return_tensors=\"pt\").input_ids.to(model.device)\n",
    "logits = ds_model.generate(input_ids, do_sample=True, max_length=100)\n",
    "tokenizer.decode(logits[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the performance and speed\n",
    "\n",
    "As the last step, we want to take a detailed look at the performance of our optimized model. Applying optimization techniques, like graph optimizations or mixed-precision, not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off.\n",
    "\n",
    "Let's test the performance (latency) of our optimized model. We will use the same generation args as for our vanilla model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 128\n",
      "DeepSpeed model: P95 latency (ms) - 6598.340023399965; Average latency (ms) - 6584.43 +\\- 10.13;\n"
     ]
    }
   ],
   "source": [
    "payload = (\n",
    "    \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    "    * 2\n",
    ")\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# generation arguments\n",
    "generation_args = dict(do_sample=False, num_beams=1, min_length=128, max_new_tokens=128)\n",
    "ds_results = measure_latency(ds_model, tokenizer, payload, generation_args, ds_model.module.device)\n",
    "\n",
    "print(f\"DeepSpeed model: {ds_results[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input payload: \n",
      " \n",
      "Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.56 GiB total capacity; 11.36 GiB already allocated; 2.44 MiB free; 11.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/transformers-deepspeed/inference/gpt_optimzation.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bt4/home/ubuntu/transformers-deepspeed/inference/gpt_optimzation.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(payload,return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bt4/home/ubuntu/transformers-deepspeed/inference/gpt_optimzation.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput payload: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mpayload\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bt4/home/ubuntu/transformers-deepspeed/inference/gpt_optimzation.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m logits \u001b[39m=\u001b[39m ds_model\u001b[39m.\u001b[39;49mgenerate(input_ids, do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, num_beams\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, min_length\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bt4/home/ubuntu/transformers-deepspeed/inference/gpt_optimzation.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtokenizer\u001b[39m.\u001b[39mdecode(logits[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()[\u001b[39mlen\u001b[39m(input_ids[\u001b[39m0\u001b[39m]):])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/transformers/generation_utils.py:1360\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1357\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1358\u001b[0m     )\n\u001b[1;32m   1359\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1361\u001b[0m         input_ids,\n\u001b[1;32m   1362\u001b[0m         beam_scorer,\n\u001b[1;32m   1363\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1364\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1365\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1366\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1367\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1368\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1369\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1370\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1374\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1376\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1377\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         renormalize_logits\u001b[39m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1382\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/transformers/generation_utils.py:2208\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2208\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2209\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2210\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2211\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2212\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2213\u001b[0m )\n\u001b[1;32m   2215\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2216\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1126\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1128\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1130\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:821\u001b[0m, in \u001b[0;36mGPTJForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    819\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 821\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    822\u001b[0m     input_ids,\n\u001b[1;32m    823\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    824\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    825\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    826\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    827\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    828\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    829\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    830\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    831\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    832\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    833\u001b[0m )\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    836\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:676\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    668\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    669\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    670\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m         head_mask[i],\n\u001b[1;32m    674\u001b[0m     )\n\u001b[1;32m    675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 676\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    677\u001b[0m         hidden_states,\n\u001b[1;32m    678\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    679\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    680\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    681\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    682\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    683\u001b[0m     )\n\u001b[1;32m    685\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/transformer/inference/transformer_inference.py:848\u001b[0m, in \u001b[0;36mDeepSpeedTransformerInference.forward\u001b[0;34m(self, input, input_mask, attention_mask, head_mask, layer_past, get_key_value, get_present, encoder_output, enc_dec_attn_mask, encoder_hidden_states, encoder_attention_mask, use_cache, alibi, output_attentions, layer_head_mask, past_key_value)\u001b[0m\n\u001b[1;32m    846\u001b[0m presents \u001b[39m=\u001b[39m (key, value)\n\u001b[1;32m    847\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_past \u001b[39m=\u001b[39m presents \u001b[39mif\u001b[39;00m layer_past \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(attention_output, \u001b[39minput\u001b[39;49m, inp_norm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention\u001b[39m.\u001b[39;49mattn_ob)\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpre_layer_norm:\n\u001b[1;32m    851\u001b[0m     ds_layernorm \u001b[39m=\u001b[39m inference_cuda_module\u001b[39m.\u001b[39mlayer_norm_fp16 \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mq_int8 \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    852\u001b[0m                             inference_cuda_module\u001b[39m.\u001b[39mlayer_norm_fp32\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/transformer/inference/transformer_inference.py:708\u001b[0m, in \u001b[0;36mDeepSpeedMLP.forward\u001b[0;34m(self, input, residual, residual_norm, bias)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, residual, residual_norm, bias):\n\u001b[0;32m--> 708\u001b[0m     \u001b[39mreturn\u001b[39;00m DeepSpeedMLPFunction\u001b[39m.\u001b[39;49mapply(\u001b[39minput\u001b[39;49m,\n\u001b[1;32m    709\u001b[0m                                       residual,\n\u001b[1;32m    710\u001b[0m                                       residual_norm,\n\u001b[1;32m    711\u001b[0m                                       bias,\n\u001b[1;32m    712\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minter_w,\n\u001b[1;32m    713\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minter_b,\n\u001b[1;32m    714\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_nw,\n\u001b[1;32m    715\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_nb,\n\u001b[1;32m    716\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    717\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmp_group,\n\u001b[1;32m    718\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_b,\n\u001b[1;32m    719\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_w,\n\u001b[1;32m    720\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_scales,\n\u001b[1;32m    721\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_groups,\n\u001b[1;32m    722\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmerge_count,\n\u001b[1;32m    723\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_gemm_func,\n\u001b[1;32m    724\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfused_gemm_gelu,\n\u001b[1;32m    725\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvector_matmul_func,\n\u001b[1;32m    726\u001b[0m                                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_residual_func)\n",
      "File \u001b[0;32m~/miniconda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/transformer/inference/transformer_inference.py:606\u001b[0m, in \u001b[0;36mDeepSpeedMLPFunction.forward\u001b[0;34m(ctx, input, residual, residual_norm, bias, inter_w, inter_b, attn_nw, attn_nb, config, mp_group, output_b, output_w, q_scales, q_groups, merge_count, mlp_gemm_func, fused_gemm_gelu, vector_matmul_func, bias_residual_func, activation_func_type)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx,\n\u001b[1;32m    584\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m             bias_residual_func,\n\u001b[1;32m    603\u001b[0m             activation_func_type\u001b[39m=\u001b[39mActivationFuncType\u001b[39m.\u001b[39mGELU):\n\u001b[1;32m    605\u001b[0m     \u001b[39mif\u001b[39;00m attn_nw \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m         output \u001b[39m=\u001b[39m fused_gemm_gelu(residual_norm,\n\u001b[1;32m    607\u001b[0m                                  inter_w,\n\u001b[1;32m    608\u001b[0m                                  inter_b,\n\u001b[1;32m    609\u001b[0m                                  output_w,\n\u001b[1;32m    610\u001b[0m                                  config\u001b[39m.\u001b[39;49mepsilon,\n\u001b[1;32m    611\u001b[0m                                  config\u001b[39m.\u001b[39;49mpre_layer_norm,\n\u001b[1;32m    612\u001b[0m                                  \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    613\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m         output, residual_add \u001b[39m=\u001b[39m mlp_gemm_func(\u001b[39minput\u001b[39m,\n\u001b[1;32m    615\u001b[0m                                          residual,\n\u001b[1;32m    616\u001b[0m                                          bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m                                          config\u001b[39m.\u001b[39mq_int8,\n\u001b[1;32m    628\u001b[0m                                          config\u001b[39m.\u001b[39mmlp_act_func_type)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.56 GiB total capacity; 11.36 GiB already allocated; 2.44 MiB free; 11.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "payload = \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(payload,return_tensors=\"pt\").input_ids.to(model.device)\n",
    "print(f\"input payload: \\n \\n{payload}\")\n",
    "logits = ds_model.generate(input_ids, do_sample=False, num_beams=2, min_length=64, max_new_tokens=64)\n",
    "\n",
    "print(f\"prediction: \\n \\n {tokenizer.decode(logits[0].tolist()[len(input_ids[0]):])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Optimized DeepsPeed model achieves latency of `6.5s` for `128` tokens or `50ms/token`.\n",
    "\n",
    "We managed to accelerate the `GPT-J-6B` model latency from `8.9s` to `6.5` for generating `128` tokens. This results into an improvement from `69ms/token` to `50ms/token` or 1.38x.\n",
    "\n",
    "![gpt-j-latency](../assets/gpt-j-inference-latency.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We successfully optimized our GPT-J Transformers with DeepSpeed-inference and managed to decrease our model latency from 69ms/token to 50ms/token or 1.3x.\n",
    "The results are impressive, but applying the optimization was as easy as adding one additional call to `deepspeed.init_inference`. \n",
    "But I have to say that this isn't a plug-and-play process you can transfer to any Transformers model, task, or dataset. Also, make sure to check if your model is compatible with DeepSpeed-Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6dd96c16031089903d5a31ec148b80aeb0d39c32affb1a1080393235fbfa2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
