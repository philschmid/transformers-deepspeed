{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate BERT & RoBERTa inference with DeepSpeed-Inference\n",
    "\n",
    "In this session, you will learn how to optimize Hugging Face Transformers models for GPU inference using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/). The session will show you how to apply state-of-the-art optimization techniques using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/). \n",
    "This session will focus on single GPU inference on BERT and RoBERTa models. \n",
    "By the end of this session, you will know optimize your Hugging Face Transformers models (BERT, RoBERTa) using DeepSpeed-Inference. We are going to optimize a BERT large model for token classification, which was fine-tuned on the conll2003 dataset to decrease the latency from 30ms to 10ms for a sequence lenght of 128.\n",
    "\n",
    "You will learn how to:\n",
    "1. [Setup Development Environment](#1-Setup-Development-Environment)\n",
    "2. [Load vanilla BERT model and set baseline](#2-Load-vanilla-BERT-model-and-set-baseline)\n",
    "3. [Optimize BERT for GPU using DeepSpeeds `InferenceEngine`](#3-Optimize-BERT-for-GPU-using-DeepSpeeds-InferenceEngine)\n",
    "4. [Evaluate the performance and speed](#4-Evaluate-the-performance-and-speed)\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "_This tutorial was created and run on an g4dn.xlarge AWS EC2 Instance including a NVIDIA T4._\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Intro: What is DeepSpeed-Inference\n",
    "\n",
    "[DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/) is an extension of the [DeepSpeed](https://www.deepspeed.ai/) framework focused on inference workloads.  [DeepSpeed Inference](https://www.deepspeed.ai/#deepspeed-inference) combines model parallelism technology such as tensor, pipeline-parallelism, with custom optimzed cuda kernels.\n",
    "DeepSpeed provides a seamless inference mode for compatible transformer based models trained using DeepSpeed, Megatron, and HuggingFace. For list of compatible models please see [here](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/replace_policy.py).\n",
    "As mentioned DeepSpeed-Inference integrates model-parallelism techniques allowing you so run multi-GPU inference for LLM, like [BLOOM](https://huggingface.co/bigscience/bloom) with 176 billion parameters.\n",
    "If you want to learn more about DeepSpeed inference: \n",
    "* [Paper: DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale](https://arxiv.org/pdf/2207.00032.pdf)\n",
    "* [Blog: Accelerating large-scale model inference and training via system optimizations and compression](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Deepspeed, along with PyTorch, Transfromers and some other libraries. Running the following cell will install all the required packages.\n",
    "\n",
    "_Note: You need a machine with a GPU and a compatible CUDA installed. You can check this by running `nvidia-smi` in your terminal. If you have a correct environment you should statistics abour your GPU._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.11.0 torchvision==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113 --upgrade -q \n",
    "!pip install deepspeed==0.7.0 --upgrade -q \n",
    "!pip install transformers[sentencepiece]==4.21.1 --upgrade -q \n",
    "!pip install datasets evaluate[evaluator]==0.2.2 seqeval --upgrade -q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start. Lets make sure we all packages are installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch \n",
    "\n",
    "# check deepspeed installation\n",
    "report = !python3 -m deepspeed.env_report\n",
    "r = re.compile('.*ninja.*OKAY.*')\n",
    "assert any(r.match(line) for line in report) == True, \"DeepSpeed Inference not correct installed\"\n",
    "\n",
    "# check cuda and torch version\n",
    "torch_version, cuda_version = torch.__version__.split(\"+\")\n",
    "torch_version = \".\".join(torch_version.split(\".\")[:2])\n",
    "cuda_version = f\"{cuda_version[2:4]}.{cuda_version[4:]}\"\n",
    "r = re.compile(f'.*torch.*{torch_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Torch version\"\n",
    "r = re.compile(f'.*cuda.*{cuda_version}.*')\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Cuda version\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load vanilla BERT model and set baseline\n",
    "\n",
    "After we setup our environment we create a baseline for our model. We use the [dslim/bert-large-NER](https://huggingface.co/dslim/bert-large-NER) a fine-tuned BERT-large model on on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset achieving an f1 score `95.7%`.\n",
    "\n",
    "To create our baseline we load the model with `transformers` and create a `token-classification` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9971501, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.9986046, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Model Repository on huggingface.co\n",
    "model_id=\"dslim/bert-large-NER\"\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_id)\n",
    "\n",
    "# Create a pipeline for token classification\n",
    "token_clf = pipeline(\"token-classification\", model=model, tokenizer=tokenizer,device=0)\n",
    "\n",
    "# Test pipeline\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "ner_results = token_clf(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Baseline with `evaluate` using the `evaluator` and the `conll2003` dataset. The Evaluator class allows us to evaluate a model/pipeline on a dataset using a defined metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluator\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load eval dataset\n",
    "eval_dataset = load_dataset(\"conll2003\", split=\"validation\")\n",
    "\n",
    "# define evaluator\n",
    "task_evaluator = evaluator(\"token-classification\")\n",
    "\n",
    "# run baseline\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=token_clf,\n",
    "    data=eval_dataset,\n",
    "    metric=\"seqeval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1 score for our model is 95.76\n",
      "The avg. Latency of the model is 18.70ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall f1 score for our model is {results['overall_f1']*100:.2f}%\")\n",
    "print(f\"The avg. Latency of the model is {results['latency_in_seconds']*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model achieves an f1 score of `95.8%` on the CoNLL-2003 dataset with a average latency across the dataset of `18.9ms`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimize BERT for GPU using DeepSpeeds `InferenceEngine`\n",
    "\n",
    "init: https://deepspeed.readthedocs.io/en/latest/inference-init.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification,pipeline\n",
    "from transformers import pipeline\n",
    "from deepspeed.module_inject import HFBertLayerPolicy\n",
    "import deepspeed\n",
    "\n",
    "# Model Repository on huggingface.co\n",
    "model_id=\"dslim/bert-large-NER\"\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_id)\n",
    "\n",
    "# init deepspeed inference engine\n",
    "ds_model = deepspeed.init_inference(\n",
    "    model=model,      # Transformers models\n",
    "    mp_size=1,        # Number of GPU\n",
    "    dtype=torch.half, # dtype of the weights (fp16)\n",
    "    # injection_policy={\"BertLayer\" : HFBertLayerPolicy}, # replace BertLayer with DS HFBertLayerPolicy\n",
    "    replace_method=\"auto\",\n",
    "    replace_with_kernel_inject=True, # replace the model with the kernel injector\n",
    ")\n",
    "\n",
    "# create acclerated pipeline\n",
    "ds_clf = pipeline(\"token-classification\", model=ds_model, tokenizer=tokenizer,device=0)\n",
    "\n",
    "# Test pipeline\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "ner_results = ds_clf(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect our model graph to see that the vanilla `BertLayer` has been replaced with a `HFBertLayer`, which includes the `DeepSpeedTransformerInference` module, a custom `nn.Module` that is optimized for inference by the DeepSpeed Team.\n",
    "\n",
    "```python\n",
    "InferenceEngine(\n",
    "  (module): BertForTokenClassification(\n",
    "    (bert): BertModel(\n",
    "      (embeddings): BertEmbeddings(\n",
    "        (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
    "        (position_embeddings): Embedding(512, 1024)\n",
    "        (token_type_embeddings): Embedding(2, 1024)\n",
    "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
    "        (dropout): Dropout(p=0.1, inplace=False)\n",
    "      )\n",
    "      (encoder): BertEncoder(\n",
    "        (layer): ModuleList(\n",
    "          (0): DeepSpeedTransformerInference(\n",
    "            (attention): DeepSpeedSelfAttention()\n",
    "            (mlp): DeepSpeedMLP()\n",
    "          )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed.ops.transformer.inference import DeepSpeedTransformerInference\n",
    "\n",
    "assert isinstance(ds_model.module.bert.encoder.layer[0], DeepSpeedTransformerInference) == True, \"Model not sucessfully initalized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run the same evaluation as for our baseline transformers model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-e6a03793369f69f7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall f1 score for our model is 95.64\n",
      "The avg. Latency of the model is 9.33ms\n"
     ]
    }
   ],
   "source": [
    "# run baseline\n",
    "ds_results = task_evaluator.compute(\n",
    "    model_or_pipeline=ds_clf,\n",
    "    data=eval_dataset,\n",
    "    metric=\"seqeval\",\n",
    ")\n",
    "\n",
    "print(f\"Overall f1 score for our model is {ds_results['overall_f1']*100:.2f}%\")\n",
    "print(f\"The avg. Latency of the model is {ds_results['latency_in_seconds']*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DeepSpeed model achieves an f1 score of `95.6%` on the CoNLL-2003 dataset with a average latency across the dataset of `9.33ms`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the performance and speed\n",
    "\n",
    "As the last step, we want to take a detailed look at the performance and accuracy of our optimized model. Applying optimization techniques, like graph optimizations or mixed-precision not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off.\n",
    "\n",
    "In our example did we achieve on the `conll2003` evaluation dataset an f1 score of `95.8%` with an average latency of `18.9ms` for the vanilla model and our optimized model an f1 score of `95.6%` with an average latency of `9.33ms`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimized ds-model achieves 99.88% accuracy of the vanilla transformers model.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The optimized ds-model achieves {round(ds_results['overall_f1']/results['overall_f1'],4)*100:.2f}% accuracy of the vanilla transformers model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized ds-model achieves `99.88%` accuracy of the vanilla transformers model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the performance (latency) of our optimized model. We are going to use a payload with a sequence length of 128 for the benchmark. To keep it simple, we are going to use a python loop and calculate the avg,mean & p95 latency for our vanilla model and for the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 128\n",
      "Vanilla model: P95 latency (ms) - 30.401047450277474; Average latency (ms) - 29.68 +\\- 0.54;\n",
      "Optimized model: P95 latency (ms) - 10.401162500056671; Average latency (ms) - 10.10 +\\- 0.17;\n",
      "Improvement through optimization: 2.92x\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(payload)\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        _ = pipe(payload)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "vanilla_model=measure_latency(token_clf)\n",
    "ds_opt_model=measure_latency(ds_clf)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Optimized model: {ds_opt_model[0]}\")\n",
    "print(f\"Improvement through optimization: {round(vanilla_model[1]/ds_opt_model[1],2)}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to accelerate the `BERT-Large` model latency from `30.4ms` to `10.40ms` or 2.92x for sequence length of 128.\n",
    "\n",
    "![bert-latency](../assets/bert-inference-latency.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We successfully optimized our BERT-large Transformers with DeepSpeed-inference and managed to decrease our model latency from 30.4ms to 10.4ms or 2.92x while keeping 99.88% of the model accuracy. \n",
    "The results are not only impressive but applying the optimzation was as easy as adding 1 additional call to `deepspeed.init_inference`. \n",
    "But I have to say that this isn't a plug and play process you can transfer to any Transformers model, task or dataset. Also make sure to check if your model is compatible with DeepSpeed-Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6dd96c16031089903d5a31ec148b80aeb0d39c32affb1a1080393235fbfa2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
